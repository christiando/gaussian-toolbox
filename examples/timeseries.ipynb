{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f42fc9f",
   "metadata": {},
   "source": [
    "$\\newcommand{\\EE}{\\mathbb{E}}$\n",
    "\n",
    "# State-space models\n",
    "\n",
    "We will demonstrate the usefulness of `GT` on a range of _state-space-models_.\n",
    "\n",
    "*Disclaimer*: This is just to showcase the power of `GT` and by no means it provides all the functionalities dedicated toolboxes like [Dynamax](https://github.com/probml/dynamax).\n",
    "\n",
    "## Example: The Kalman Filter\n",
    "\n",
    "A famous example for timeseries model is the Kalman Filter (and also Smoother). Inference and learning with this model requires exactly the features, that `GT` leverages. Let's see that in a bit more detail. \n",
    "\n",
    "The Kalman Filter is a two layered model described by the following equations: \n",
    "\n",
    "$$\n",
    "\\color{red}{Z_{t} = A Z_{t-1} + b + \\zeta_t} \\\\\n",
    "\\color{blue}{X_{t} = C Z_t + d + \\xi_t},\n",
    "$$\n",
    "\n",
    "where $X_t$ are our observations and $Z_t$ latent (unobserved) variables. Furthermore, the noise variables are\n",
    "\n",
    "$$\n",
    "\\color{red}{\\zeta_t \\sim N(0,\\Sigma_z)}\\\\\n",
    "\\color{blue}{\\xi_t \\sim N(0,\\Sigma_x)}.\n",
    "$$\n",
    "\n",
    "Hence, our model is composed by a $\\color{red}{\\text{state model}}$ and an $\\color{blue}{\\text{emission- or observation model}}$.\n",
    "\n",
    "The joint likelihood is given by\n",
    "\n",
    "$$\n",
    "p(Z_{0:T}, X_{1:T}\\vert w) = p(Z_0\\vert w)\\prod_{t=0}^{T-1} \\color{blue}{p(X_{t}\\vert Z_{t}, w)}\\color{red}{p(Z_{t}\\vert Z_{t-1}, w)}.\n",
    "$$\n",
    "\n",
    "where the parameters $w$ are $A,b,\\Sigma_z, C, d, \\Sigma_x$ and the mean and covariance of the initial distribution over $Z_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae46ac",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We call inference getting the posterior over the latent variables $Z_{0:T}$ given a set of observations $X_{1:T}$. For the Kalman filter this is done in a two-step procedure.\n",
    "\n",
    "#### Filter procedure:\n",
    "\n",
    "Assume that you have the estimate for $\\color{darkorange}{p(Z_t\\vert X_{1:t})}$.\n",
    "\n",
    "+ Prediction step:\n",
    "\n",
    "$$\n",
    " \\color{green}{p(Z_{t+1}\\vert X_{1:t})} = \\int \\color{red}{p(Z_{t+1}Z_t)}\\color{darkorange}{p(Z_t\\vert X_{1:t})}{\\rm d} Z_t\n",
    "$$\n",
    "\n",
    "Has the form of an _affine marginal transformation_ $T_{\\rm marg}[p(Y|X),p(X)] = p(Y)$.\n",
    "\n",
    "+ Filter step:\n",
    "\n",
    "$$\n",
    " \\color{darkorange}{p(Z_{t+1}\\vert X_{1:t+1})} = \\frac{\\color{blue}{p(X_{t+1}|Z_{t+1})}\\color{green}{p(Z_{t+1}\\vert X_{1:t})}}{\\int \\color{blue}{p(X_{t+1}|Z^\\prime_{t+1})}\\color{green}{p(Z^\\prime_{t+1}\\vert X_{1:t})}{\\rm d} Z^\\prime_{t+1}}\n",
    "$$\n",
    "\n",
    "\n",
    "Hase the form of a _affine conditional transformation_: $T_{\\rm cond}[p(Y|X),p(X)] = p(X\\vert Y)$. This procedure can iterate forward through time.\n",
    "\n",
    "#### Smoothing procedure:\n",
    "\n",
    "_Smoothing_ is called the backward iteration through time, which yields the density over the trajectories of the latent variables $Z_{0:T}$ given all observations (past __and__ future). This involves also just affine transformations for the Kalman Filter.\n",
    "\n",
    "Let's generate some example data, to show case how this is done in the `GT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "import objax\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "T = 500\n",
    "trange = jnp.arange(T)\n",
    "Dx = 2\n",
    "Dz = 2\n",
    "X = jnp.empty((T,Dx))\n",
    "X = X.at[:,0].set(jnp.sin(trange / 20))\n",
    "X = X.at[:,1].set(jnp.sin(trange / 10))\n",
    "noise_x = .2\n",
    "noise_z = .1\n",
    "X += noise_x * objax.random.normal(X.shape)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax1 = plt.subplot(211)\n",
    "plt.plot(X[:,0], 'k.')\n",
    "plt.xlim([0,500])\n",
    "plt.ylabel('$x_1$')\n",
    "plt.subplot(212, sharex=ax1)\n",
    "plt.plot(X[:,1], 'k.')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c6a07",
   "metadata": {},
   "source": [
    "### Implementation in `GT`\n",
    "\n",
    "`GT` provides a `timeseries` module, where a couple of `state_model`s and `observation_model`s and are implemented. They all can be combined to construct different state-space models, and all their parameters can be learnt via an EM procedure (exact or in some cases approximate).\n",
    "\n",
    "Let's see how this is working for the Kalman filter.\n",
    "\n",
    "We previously saw that the state and observation model are both linear for the Kalman filter. Hence, we will create a `LinearStateModel` and `LinearObservationModel`, and combine them to a `StateSpaceModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_toolbox.timeseries import state_model, observation_model\n",
    "from gaussian_toolbox.timeseries.ssm import StateSpaceModel\n",
    "\n",
    "Dz = 2\n",
    "Dx = X.shape[1]\n",
    "\n",
    "om = observation_model.LinearObservationModel(Dx=Dx, Dz=Dz)\n",
    "sm = state_model.LinearStateModel(Dz=Dz, noise_z=.5)\n",
    "\n",
    "ssm = StateSpaceModel(X, om, sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7338717",
   "metadata": {},
   "source": [
    "Now let's perform filtering (`forward_sweep`) and smoothing (`backward_sweep`) on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm.forward_sweep()\n",
    "\n",
    "filter_mean = ssm.filter_density.mu\n",
    "filter_std = jnp.sqrt(ssm.filter_density.Sigma.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "ssm.backward_sweep()\n",
    "\n",
    "smoothing_mean = ssm.smoothing_density.mu\n",
    "smoothing_std = jnp.sqrt(ssm.smoothing_density.Sigma.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax1 = plt.subplot(211)\n",
    "plt.fill_between(range(0,T+1), filter_mean[:,0] - filter_std[:,0], filter_mean[:,0] + filter_std[:,0], alpha=.3, label='filter', color='C4')\n",
    "plt.fill_between(range(0,T+1), smoothing_mean[:,0] - smoothing_std[:,0], smoothing_mean[:,0] + smoothing_std[:,0], alpha=.3, label='smoothing', color='C1')\n",
    "plt.plot(range(0,T+1), filter_mean[:,0], filter_mean[:,0], color='C4', ls='--')\n",
    "plt.plot(range(0,T+1), smoothing_mean[:,0], smoothing_mean[:,0], color='C1')\n",
    "plt.xlim([0,500])\n",
    "plt.ylabel('$Z_1$')\n",
    "plt.legend()\n",
    "plt.title('Latent variables')\n",
    "plt.subplot(212, sharex=ax1)\n",
    "plt.fill_between(range(0,T+1), filter_mean[:,1] - filter_std[:,1], filter_mean[:,1] + filter_std[:,1], alpha=.3, color='C4')\n",
    "plt.fill_between(range(0,T+1), smoothing_mean[:,1] - smoothing_std[:,1], smoothing_mean[:,1] + smoothing_std[:,1], alpha=.3, color='C1')\n",
    "plt.plot(range(0,T+1), filter_mean[:,1], filter_mean[:,1], color='C4', ls='--')\n",
    "plt.plot(range(0,T+1), smoothing_mean[:,1], smoothing_mean[:,1], color='C1')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('$Z_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e020591",
   "metadata": {},
   "source": [
    "We see that the smoothing density has narrower confidence intervals, compared to the filter density, because it contains also future information. Now let's check the estimate of the data density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x = ssm.compute_data_density()\n",
    "mean_estimate = p_x.mu\n",
    "std_estimate = 2 * jnp.sqrt(p_x.Sigma.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax1 = plt.subplot(211)\n",
    "plt.title('Observed variables')\n",
    "plt.plot(X[:,0], 'k.')\n",
    "plt.fill_between(range(1,T), mean_estimate[1:,0] - std_estimate[1:,0], mean_estimate[1:,0] + std_estimate[1:,0], alpha=.5)\n",
    "plt.xlim([0,500])\n",
    "plt.ylabel('$x_1$')\n",
    "plt.subplot(212, sharex=ax1)\n",
    "plt.plot(X[:,1], 'k.')\n",
    "plt.fill_between(range(1,T), mean_estimate[1:,1] - std_estimate[1:,1], mean_estimate[1:,1] + std_estimate[1:,1], \n",
    "                    alpha=.5)\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fffe9d0",
   "metadata": {},
   "source": [
    "We see that the confidence intervals are too wide. This is because we did not learn the model parameters $w$ yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2a1ba",
   "metadata": {},
   "source": [
    "### Learning parameters with Expectation Maximization\n",
    "\n",
    "We do not maximize the likelihood $p(x_{1:T}\\vert w)$, but the Q-function which is defined as \n",
    "\n",
    "$$\n",
    " Q(w,w^{\\rm old}) = \\EE_{p(z_{0:t}\\vert x_{1:T}, w^{\\rm old})}\\left[\\ln p(z_{0:T}, x_{1:T}\\vert w)\\right] \\leq \\ln p(x_{1:T}\\vert w).\n",
    "$$\n",
    "\n",
    "This nicely decomposes into\n",
    "\n",
    "$$\n",
    "    Q(w,w^{\\rm old}) = \\sum_{t=1}^T\\EE_{p(z_t\\vert x_{1:T}, w^{\\rm old})}\\left[\\ln \\color{blue}{p(x_t\\vert z_{t}, w)}\\right] + \\sum_{t=1}^T\\EE_{p(z_{t-1:t}\\vert x_{1:T}, w^{\\rm old})}\\left[\\ln \\color{red}{p(z_t\\vert z_{t-1},w)}\\right] + \\EE_{p(z_0\\vert x_{1:T}, w^{\\rm old})}\\left[\\ln p(z_0\\vert w)\\right].\n",
    "$$\n",
    "\n",
    "__Important remark__: $\\color{blue}{\\text{Observation model}}$ and $\\color{red}{\\text{state model}}$ are always decoupled!\n",
    "\n",
    "```p_X.integrate('...')``` function is very useful to compute the Q-function easily. \n",
    "\n",
    "Now we will use the Expectation maximization (EM) algorithm, where we infer the density over trajectories as we have seen before (aka E-Step). And then given that density we maximize the $Q$ function with respect to parameters $w$.\n",
    "\n",
    "Let's see how this is done in `GT`. We just need to invoke `ssm.fit()`, and the model will use the EM procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18285d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm.fit()\n",
    "plt.plot(jnp.arange(1, len(ssm.llk_list) + 1), ssm.llk_list, 'k')\n",
    "plt.xlabel('EM iterations')\n",
    "plt.ylabel('Log likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910bfd8",
   "metadata": {},
   "source": [
    "Let's check the data fit again. We see that this looks much better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_x = ssm.compute_data_density() # int p(x|z)p(z)dz\n",
    "mean_estimate = p_x.mu\n",
    "std_estimate = 2 * jnp.sqrt(p_x.Sigma.diagonal(axis1=1, axis2=2))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "ax1 = plt.subplot(211)\n",
    "plt.plot(X[:,0], 'k.')\n",
    "plt.fill_between(range(1,T), mean_estimate[1:,0] - std_estimate[1:,0], mean_estimate[1:,0] + std_estimate[1:,0], alpha=.5)\n",
    "plt.xlim([0,500])\n",
    "plt.ylabel('$x_1$')\n",
    "plt.subplot(212, sharex=ax1)\n",
    "plt.plot(X[:,1], 'k.')\n",
    "plt.fill_between(range(1,T), mean_estimate[1:,1] - std_estimate[1:,1], mean_estimate[1:,1] + std_estimate[1:,1], \n",
    "                    alpha=.5)\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b7d17",
   "metadata": {},
   "source": [
    "## Model overview\n",
    "\n",
    "While we saw just the simplest state-space model with everything linear, `GT` provides a range of state- and obsevration models, which include also non-linearities, or are designed for heteroscedastic data. \n",
    "\n",
    "__Available state models__\n",
    "\n",
    "| Model name | State equation | Short description | Approximation |\n",
    "|---|---|---|---|\n",
    "| `LinearStateModel` | $$X_t = AZ_t + b + \\zeta_t$$ | Linear mean |Exact|\n",
    "| `LRBFMStateModel` | $$X_t = A\\phi(Z_t) + b + \\zeta_t$$ | Mean with linear and radial basis function features| Moment matching|\n",
    "| `LSEMStateModel` | $$X_t = A\\phi(Z_t) + b + \\zeta_t$$ | Mean with linear and squared exponential features|Moment matching|\n",
    "| `NNControlStateModel` |$$X_t = A(u_t)Z_t + d(u_t) + \\zeta(Z_t)$$| Mean depending on control variables through a network|Exact|\n",
    "\n",
    "__Available observation models__\n",
    "\n",
    "| Model name | Observation equation | Short description | Approximation |\n",
    "|---|---|---|---|\n",
    "| `LinearObservationModel` | $X_t = CZ_t + d + \\xi_t$ | Linear mean |Exact|\n",
    "| `LRBFMObservationModel` | $$X_t = C\\phi(Z_t) + d + \\xi_t$$ | Mean with linear and radial basis function features|Moment matching|\n",
    "| `LSEMObservationModel` | $$X_t = C\\phi(Z_t) + d + \\xi_t$$ | Mean with linear and squared exponential features|Moment matching|\n",
    "| `HCCovObservationModel` |$$X_t = CZ_t + d + \\xi(Z_t)$$| Linear mean, but _heteroscedastic_ noise|Moment matching & lower bound for $Q$|\n",
    "\n",
    "And the nice thing is, that any state model can be combined with any observation model. (As a recommendation for starting take one of the two still Linear before getting too complex.) Overall fitting time will increase with the complexity of the models you choose (Linear model is fastest).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gaussian_toolbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "782cd5dadb26a54d425c88a99e91aaf22c15a8b7b364f1b6867d338206b7ed04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
