{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55007fce",
   "metadata": {},
   "source": [
    "# Kalman Filter\n",
    "\n",
    "We will demonstrate the usefulness of `GT` on the classical [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter).\n",
    "\n",
    "## Example: The Kalman Filter\n",
    "\n",
    "A famous example for timeseries model is the Kalman Filter (and also Smoother). Inference and learning with this model requires exactly the features, that `GT` leverages. Let's see that in a bit more detail. \n",
    "\n",
    "The Kalman Filter is a two layered model described by the following equations: \n",
    "\n",
    "$$\n",
    "\\color{red}{Z_{t} = A Z_{t-1} + b + \\zeta_t} \\\\\n",
    "\\color{blue}{X_{t} = C Z_t + d + \\xi_t},\n",
    "$$\n",
    "\n",
    "where $X_t$ are our observations and $Z_t$ latent (unobserved) variables. Furthermore, the noise variables are\n",
    "\n",
    "$$\n",
    "\\color{red}{\\zeta_t \\sim N(0,\\Sigma_z)}\\\\\n",
    "\\color{blue}{\\xi_t \\sim N(0,\\Sigma_x)}.\n",
    "$$\n",
    "\n",
    "Hence, our model is composed by a $\\color{red}{\\text{state model}}$ and an $\\color{blue}{\\text{emission- or observation model}}$, which are two conditional Gaussian densities. Lets create these objects in `GT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_toolbox import pdf, conditional\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from jax import lax\n",
    "\n",
    "Dz = Dx = 2\n",
    "A, b = jnp.array([jnp.eye(Dz) - jnp.array([[.01, -.1], [.1, .01]])]), jnp.zeros((1,Dz))\n",
    "Sigma_z = .001 * jnp.array([jnp.eye(Dz)])\n",
    "C, d = jnp.array([jnp.eye(Dz)]), jnp.zeros((1,Dz))\n",
    "Sigma_x = .01 * jnp.array([jnp.eye(Dz)])\n",
    "\n",
    "state_density = conditional.ConditionalGaussianPDF(M=A, b=b, Sigma=Sigma_z)\n",
    "observation_density = conditional.ConditionalGaussianPDF(M=C, b=d, Sigma=Sigma_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbae830a",
   "metadata": {},
   "source": [
    "Now we would like to sample data from the generative process, that is depicted by this two objects. For this we need an additional density, the initial density. Then we define a function that does one sample step and iterate that forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create initial density\n",
    "mu0, Sigma0 = jnp.zeros((1, Dz)), jnp.array([jnp.eye(Dz)])\n",
    "p_z0 = pdf.GaussianPDF(Sigma=Sigma0, mu=mu0)\n",
    "\n",
    "\n",
    "T = 500 # Time steps to sample\n",
    "### Create PRNGs for sampling\n",
    "key = random.PRNGKey(0)\n",
    "subkey, key = random.split(key)\n",
    "z0 = p_z0.sample(subkey, 1)[0]\n",
    "range_keys = random.split(key, T+1)\n",
    "key, range_keys_z = range_keys[0], range_keys[1:]\n",
    "range_keys = random.split(key, T+1)\n",
    "key, range_keys_x = range_keys[0], range_keys[1:]\n",
    "#####################################\n",
    "\n",
    "def sample_kf(z_prev, keys):\n",
    "    \"\"\"Sample one step of the Kalman filter.\"\"\"\n",
    "    key_z, key_x = keys\n",
    "    # Sample state by conditioning the state model on z_{t-1} and then sample z_{t}\n",
    "    z_cur = state_density(z_prev).sample(key_z, 1)[0]\n",
    "    # Sample observation by conditioning the observation model on z_{t} and then sample x_{t}\n",
    "    x_cur = observation_density(z_cur).sample(key_x,1)[0]\n",
    "    result = z_cur[0], x_cur[0]\n",
    "    return z_cur, result\n",
    "\n",
    "# This is just the jax way of writing a for loop\n",
    "_, result = lax.scan(sample_kf, z0, (range_keys_z, range_keys_x))\n",
    "z_sample, x_sample = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363b0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.subplot(211)\n",
    "plt.plot(x_sample[:,0],'k.')\n",
    "plt.yticks([])\n",
    "plt.ylabel('$x_1$')\n",
    "plt.subplot(212)\n",
    "plt.plot(x_sample[:,1],'k.')\n",
    "plt.yticks([])\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlabel('Time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "329dbd1e",
   "metadata": {},
   "source": [
    "Now having data at hand we would like to estimate the density over latent variables and data again. Hence, we would like to iterate forward the Kalman filter, which consists of two steps.\n",
    "\n",
    "+ Prediction step:\n",
    "\n",
    "$$\n",
    " {\\color{green} p(Z_{t+1}\\vert X_{1:t})} = \\int {\\color{red} p(Z_{t+1}Z_t)}{\\color{darkorange} p(Z_t\\vert X_{1:t})}{\\rm d} Z_t\n",
    "$$\n",
    "\n",
    "Has the form of an _affine marginal transformation_ $T_{\\rm marg}[p(Y|X),p(X)] = p(Y)$.\n",
    "\n",
    "+ Filter step:\n",
    "\n",
    "$$\n",
    " {\\color{darkorange} p(Z_{t+1}\\vert X_{1:t+1})} = \\frac{{\\color{blue} p(X_{t+1}|Z_{t+1})}{\\color{green} p(Z_{t+1}\\vert X_{1:t})}}{\\int {\\color{blue} p(X_{t+1}|Z^\\prime_{t+1})}{\\color{green} p(Z^\\prime_{t+1}\\vert X_{1:t})}{\\rm d} Z^\\prime_{t+1}}\n",
    "$$\n",
    "\n",
    "This can be done by an _affine conditional transformation_ and then condition on the observation.\n",
    "\n",
    "Similar two the sampling procedure, we can define a function that iterates forward these densities. With `GT` this can be done with a only a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f76a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_step(previous_filter_density, x_t) -> pdf.GaussianPDF:\n",
    "    r\"\"\"Perform prediction and filter step\n",
    "    \n",
    "    .. math::\n",
    "    \n",
    "        p(Z_t|X_{1:t-1}) = \\int p(Z_t|Z_{t-1})p(Z_{t-1}|X_{1:t-1}) {\\rm d}Z_{t-1}\n",
    "    :param pre_filter_density: Density :math:`p(z_t-1|x_{1:t-1})`\n",
    "    :type pre_filter_density: pdf.GaussianPDF\n",
    "    :return: Prediction density :math:`p(z_t|x_{1:t-1})`.\n",
    "    :rtype: pdf.GaussianPDF\n",
    "    \"\"\"\n",
    "    # p(z_t|x_{1:t-1})\n",
    "    prediction_density = state_density.affine_marginal_transformation(previous_filter_density)\n",
    "    p_z_given_x = observation_density.affine_conditional_transformation(prediction_density)\n",
    "    # Condition on x_t\n",
    "    cur_filter_density = p_z_given_x.condition_on_x(x_t[None])\n",
    "    result = (cur_filter_density.Sigma[0], cur_filter_density.mu[0])\n",
    "    return cur_filter_density, result\n",
    "\n",
    "_, result = lax.scan(kf_step, p_z0, (x_sample))\n",
    "Sigma_filter, mu_filter = result\n",
    "filter_density = pdf.GaussianPDF(Sigma=Sigma_filter, mu=mu_filter)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2e8a5a3",
   "metadata": {},
   "source": [
    "The Kalman filter provides the filter density over the latent state. If we want the data density we can do that in one line with `GT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_density = observation_density.affine_marginal_transformation(filter_density)\n",
    "\n",
    "std_filter = jnp.sqrt(jnp.diagonal(data_density.Sigma, axis1=-1, axis2=-2))\n",
    "upper, lower = data_density.mu + 1.98 * std_filter, data_density.mu - 1.98 * std_filter\n",
    "plt.subplot(211)\n",
    "plt.plot(x_sample[:,0],'k.')\n",
    "plt.fill_between(range(T), lower[:,0], upper[:,0], alpha=.5)\n",
    "plt.yticks([])\n",
    "plt.ylabel('$x_1$')\n",
    "plt.subplot(212)\n",
    "plt.plot(x_sample[:,1],'k.')\n",
    "plt.fill_between(range(T), lower[:,1], upper[:,1], alpha=.5)\n",
    "plt.yticks([])\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlabel('Time')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd4384d",
   "metadata": {},
   "source": [
    "Of course, this is only demonstrating how to perform Kalman-Filter for the known model. But `GT` provides also utilities to e.g. simply calculate the Q-function, which is the objective in the expectation maximization procedure [[Dempster, 1976]](https://www.eng.auburn.edu/~roppeth/courses/00sum13/7970%202013A%20ADvMobRob%20sp13/literature/paper%20W%20refs/dempster%20EM%201977.pdf) to learn the model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
