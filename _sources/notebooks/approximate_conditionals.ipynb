{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate conditionals\n",
    "\n",
    "Though restricted to Gaussian operation, `GT` can also be used for non Gaussian conditionals. The only requirement is, that the affine transformations are defined in some way, even if that is only approximately. `GT`provides some examples, where the affine transformations can be appoximated by moment matching. In this notebook we showcase these `approximate_conditionals` by a simple example. Let's consider a the infamous MNIST dataset, a dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from jax import config\n",
    "from matplotlib import pyplot as plt\n",
    "from gaussian_toolbox import approximate_conditional, pdf\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "from gaussian_toolbox.utils.jax_minimize_wrapper import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = load_digits()\n",
    "X, y = mnist['data'], mnist['target']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "N, Dx = X_scaled.shape\n",
    "\n",
    "num_data_to_show = 10\n",
    "plt.figure(figsize=(12, 6))\n",
    "for isample in range(num_data_to_show):\n",
    "    ax = plt.subplot(2, num_data_to_show//2,isample + 1)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(scaler.inverse_transform(X_scaled)[isample].reshape(8,8), cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a simple latent variable model of the form\n",
    "\n",
    "$$\n",
    "p(X\\vert \\Phi) = \\int p(X\\vert Z, \\Phi)p(Z){\\rm d} Z,\n",
    "$$\n",
    "\n",
    "where $X$ is the image data, and $Z$ some latent variable. The distribution over the latent space we consider standard normal. Let's first define this object in `GT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of latent space\n",
    "Dz = 3\n",
    "Sigma_z, mu_z = jnp.array([jnp.eye(Dz)]), jnp.zeros((1, Dz))\n",
    "p_z = pdf.GaussianPDF(Sigma=Sigma_z, mu=mu_z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it remains to clarify what is $p(X\\vert Z, \\Phi)$. Here we consider a Gaussian with non-linear mean\n",
    "$$\n",
    "\\mu(Z) = W \\phi(Z),\n",
    "$$\n",
    "where the feature vector is a vector $(1, z_1, \\ldots, z_{D_z}, f_1(Z), \\ldots, f_K(Z))^\\top$ and $f_i$ are radial basis function kernels. For this model, we can do [moment matching](https://proceedings.neurips.cc/paper/1998/hash/0ebcc77dc72360d0eb8e9504c78d38bd-Abstract.html), i.e. define the output of the affine transformation with the Gaussian that has the first two moments equal to the exact density. $\\Phi$ denotes all the parameters of the conditional ($W$, the kernel parameters, and the covariance). Let's construct this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kernels = 50\n",
    "\n",
    "params = {'log_sigma': jnp.log(jnp.array([1.])),\n",
    "          'M': jnp.array(1e-2 * np.random.randn(1, Dx, Dz + num_kernels)),\n",
    "          'b': jnp.zeros([1, Dx]),\n",
    "          'log_length_scale': jnp.array([0.]),\n",
    "          'mu': jnp.array(np.random.randn(num_kernels, Dz))}\n",
    "Sigma_x = jnp.array(jnp.eye(Dx))\n",
    "\n",
    "def create_conditional(params):\n",
    "    Sigma_x = jnp.array([jnp.exp(params['log_sigma']) * jnp.eye(Dx)])\n",
    "    Lambda_x = jnp.array([jnp.exp(-params['log_sigma']) * jnp.eye(Dx)])\n",
    "    ln_det_Sigma_x = jnp.array([params['log_sigma'] * Dx])\n",
    "    length_scale = jnp.array(jnp.exp(params['log_length_scale']) * jnp.ones([num_kernels, Dz]))\n",
    "    return approximate_conditional.LRBFGaussianConditional(M = params['M'], b = params['b'], mu=params['mu'], length_scale=length_scale, Sigma=Sigma_x, Lambda=Lambda_x, ln_det_Sigma=ln_det_Sigma_x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wish to learn the model parameters $\\Phi$ by minimizing the negative log-likelihood. The log-likelihood is given by\n",
    "\n",
    "$$\n",
    "\\ell(X,\\phi) = \\sum_i \\ln \\int p(X_i\\vert Z, \\Phi)p(Z){\\rm d} Z,\n",
    "$$\n",
    "which involves the `affine_marginal_transform`. Again with `GT` this is done with minimal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_likelihood(params, X):\n",
    "    cond_x_given_z = create_conditional(params)\n",
    "    return -jnp.mean(cond_x_given_z.affine_marginal_transformation(p_z).evaluate_ln(X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An that is it. Now we just minimize our objective and learn the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(get_neg_likelihood, params, method='L-BFGS-B', args=(X_scaled,), options={})\n",
    "p_x_given_z_opt = create_conditional(result.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the learnt model we also project the data back into the latent space, by the `affine conditional transformation` and then condition on the data (which gives us the equivalent of the encoder in a variational auto encoder (VAE))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_z_given_x = p_x_given_z_opt.affine_conditional_transformation(p_z)\n",
    "p_z_given_X = p_z_given_x(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111,projection='3d')\n",
    "for label in range(10):\n",
    "    \n",
    "    plt.plot(p_z_given_X.mu[y==label,0], p_z_given_X.mu[y==label,1], p_z_given_X.mu[y==label,2], '.', label=label, alpha=.5)\n",
    "plt.legend(title='Label')\n",
    "ax.set_xlabel('$z_1$')\n",
    "ax.set_ylabel('$z_2$')\n",
    "ax.set_zlabel('$z_3$')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the different digits are clustered int the latent space. We can also see, whether samples from the model look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "key = random.PRNGKey(42)\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "num_samples = 10\n",
    "z_sample = p_z.sample(subkey, num_samples)[:,0]\n",
    "key, subkey = random.split(key)\n",
    "x_sample = scaler.inverse_transform(p_x_given_z_opt(z_sample).sample(subkey, 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for isample in range(num_samples):\n",
    "    ax = plt.subplot(2, num_samples//2,isample + 1)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(x_sample[isample].reshape(8,8), cmap='gray')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the sample seem a bit blurry, they definitely sample some structure. But of course, we are looking at a very simple model. Maybe you can find a better one? ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "782cd5dadb26a54d425c88a99e91aaf22c15a8b7b364f1b6867d338206b7ed04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
