{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Example: Bayesian linear regression\n",
    "\n",
    "To show how the objects we have encountered so far can be used, we will consider the most basic example in Bayesian inference: _Bayesian linear regression_.\n",
    "\n",
    "We consider the model\n",
    "\n",
    "$$\n",
    "    Y \\sim {\\cal N}(W X, \\sigma^2) \\text{ (likelihood)}\\\\\n",
    "    W \\sim {\\cal N}(0,1) \\text{ (prior) }\n",
    "$$\n",
    "\n",
    "Let's generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = 0.2\n",
    "D = 1\n",
    "N = 10\n",
    "\n",
    "x_range = jnp.array([jnp.arange(0,2,.1)]).T\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, subkey = jax.random.split(key)\n",
    "x = 2 * jax.random.uniform(subkey, (N, D))\n",
    "w = 2 * jnp.ones((D,))\n",
    "b = 0 * jnp.zeros((1,))\n",
    "key, subkey = jax.random.split(key)\n",
    "y = jnp.dot(x, w) + b + sigma_y * jax.random.normal(subkey, (N,))\n",
    "y = jnp.array([y]).T\n",
    "y_range = jnp.dot(x_range, w) + b \n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(x, y, 'ks')\n",
    "plt.plot(x_range, y_range, 'k')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Let's now construct the _prior_ and the _likelihood_ in `GT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_toolbox import pdf, conditional\n",
    "\n",
    "mu_prior = 0\n",
    "sigma_prior = 1.0\n",
    "\n",
    "# Construct prior\n",
    "Sigma = jnp.array([sigma_prior ** 2.0 * jnp.eye(D)])\n",
    "mu = mu_prior * jnp.ones((1, D))\n",
    "\n",
    "# p(W)\n",
    "prior = pdf.GaussianDiagPDF(Sigma=Sigma, mu=mu) # p(W)\n",
    "\n",
    "\n",
    "sigma_y = 0.2\n",
    "\n",
    "# Construct likelihood\n",
    "Sigma_lk = jnp.ones((N, 1, 1)) * sigma_y ** 2\n",
    "M_lk = x[:, None]\n",
    "\n",
    "# p(Y|W)\n",
    "likelihood = conditional.ConditionalGaussianPDF(M=M_lk, Sigma=Sigma_lk) # p(Y_i| X=x_i, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In Bayesian inference we are interest in two particular objects. One is the _posterior_ over the model parameter $W$ given the data, and the other predictive density for new points $X=x^\\star$ given the posterior. We can get this objects with the affine transformations encountered before:\n",
    "\n",
    "1. Posterior $p(W\\vert X=x_i, Y) = T_{\\rm cond}[p(Y\\vert X=x_i, W), p(W)]$ and then condition on $Y=y_i$.\n",
    "2. Predictive posterior $p(Y^\\star\\vert X^\\star=x_j, {X=x_i, Y=y_i}) = T_{\\rm marg}[p(Y^\\star\\vert X^\\star=x_j, W), p(W\\vert X=x_i, Y=y_i)]$\n",
    "\n",
    "__Sequential inference__\n",
    "\n",
    "In the following, we iteratively update our _posterior_ and do the _prediction_ for one data point coming in at at time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial posterior = prior\n",
    "posterior = prior\n",
    "\n",
    "# Prediction\n",
    "x_star = x_range\n",
    "N_star = x_star.shape[0]\n",
    "M = jnp.reshape(x_star, (N_star, 1, x_star.shape[1]))\n",
    "b = jnp.zeros((N_star, 1))\n",
    "Sigma = sigma_y ** 2.0 * jnp.ones((N_star, 1, 1))\n",
    "predictive_conditional = conditional.ConditionalGaussianPDF(M=M, b=b, Sigma=Sigma)\n",
    "\n",
    "predictive_posterior = predictive_conditional.affine_marginal_transformation(posterior)\n",
    "\n",
    "# just plotting ############\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(x_range, y_range, 'k')\n",
    "ax1.plot(x_star, predictive_posterior.mu, 'C3--')\n",
    "ax1.fill_between(x_star[:,0], predictive_posterior.mu[:,0] - 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                              predictive_posterior.mu[:,0] + 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                alpha=.3, color='C3')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "w_range = jnp.array([jnp.arange(0,2.5,.001)]).T\n",
    "ax2.plot(w_range[:,0], posterior(w_range)[0], label='posterior')\n",
    "ax2.plot(w_range[:,0], prior(w_range)[0], label='prior')\n",
    "ax2.vlines(w,0,jnp.amax(posterior(w_range)[0]) + 1, 'k')\n",
    "ax2.set_xlabel('W')\n",
    "ax2.set_ylabel('p(W)')\n",
    "ax2.set_title('N=0')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#############################\n",
    "\n",
    "for i in range(N):\n",
    "    # Updating posterior\n",
    "    likelihood_i = likelihood.slice(jnp.array([i]))\n",
    "    # STEP 1\n",
    "    posterior_Yi = likelihood_i.affine_conditional_transformation(posterior)\n",
    "    posterior = posterior_Yi(y[i:i+1])\n",
    "    \n",
    "    # STEP 2\n",
    "    predictive_posterior = predictive_conditional.affine_marginal_transformation(posterior)\n",
    "    \n",
    "    # just plotting ############\n",
    "    clear_output(wait=True)\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    time.sleep(1.)\n",
    "    ax1.cla(), ax2.cla()\n",
    "    ax2.plot(w_range[:,0], posterior(w_range)[0], label='posterior')\n",
    "    ax2.plot(w_range[:,0], prior(w_range)[0], label='prior')\n",
    "    ax2.vlines(w,0,jnp.amax(posterior(w_range)[0]) + 1, 'k')\n",
    "    ax2.set_xlabel('W')\n",
    "    ax2.set_ylabel('p(W)')\n",
    "    ax2.set_title('N=%d' %(i+1))\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "    ax1.plot(x[:i+1], y[:i+1], 'ks')\n",
    "    ax1.plot(x_range, y_range, 'k')\n",
    "    ax1.plot(x_star, predictive_posterior.mu, 'C3--')\n",
    "    ax1.fill_between(x_star[:,0], predictive_posterior.mu[:,0] - 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                                  predictive_posterior.mu[:,0] + 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                    alpha=.3, color='C3')\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "\n",
    "    plt.show()\n",
    "    #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "__One step inference__\n",
    "\n",
    "Instead of doing sequential updates, of course we can also do the inference in one go. We will get the posterior in 4 steps.\n",
    "\n",
    "1. We create the likelihood for each data point $\\{p(Y\\vert X=x_i, W)\\}_{i=1}^N$ and set $Y=y_i$, which gets us a `ConjugateFactor`. \n",
    "2. Compute the product $\\prod_{i=1}^Np(Y=y_i\\vert X=x_i, W)$.\n",
    "3. Multiply with prior.\n",
    "4. Normalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior in one go\n",
    "# Step 1\n",
    "likelihood_yi = likelihood.set_y(y) # p(Y=y_i| W)\n",
    "# Step 2\n",
    "likelihood_y = likelihood_yi.product() # \\prod_i p(Y=y_i| W)\n",
    "# Step 3\n",
    "posterior_unnormalized = prior * likelihood_y # p(W, Y=y) = p(Y=y|W)p(W)\n",
    "# Step 4\n",
    "posterior = posterior_unnormalized.get_density() # p(W| Y=y)\n",
    "\n",
    "predictive_posterior = predictive_conditional.affine_marginal_transformation(posterior)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(122)\n",
    "plt.plot(w_range[:,0], posterior(w_range)[0], label='posterior')\n",
    "plt.plot(w_range[:,0], prior(w_range)[0], label='prior')\n",
    "plt.vlines(w,0,jnp.amax(posterior(w_range)[0]) + 1, 'k')\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('p(w)')\n",
    "plt.legend()\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(x[:i+1], y[:i+1], 'ks')\n",
    "ax1.plot(x_range, y_range, 'k')\n",
    "ax1.plot(x_star, predictive_posterior.mu, 'C3--')\n",
    "ax1.fill_between(x_star[:,0], predictive_posterior.mu[:,0] - 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                              predictive_posterior.mu[:,0] + 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                alpha=.3, color='C3')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "__Bayesian linear regression with Neural network mean__\n",
    "\n",
    "In the next example we will learn how neural networks, and learning thereof, can seemlessly be integrated with `GT`.\n",
    "\n",
    "Let's consider a slightly more complex example, where we have some non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = 0.2\n",
    "D = 1\n",
    "N = 20\n",
    "\n",
    "f = lambda x: jnp.cos(2 * jnp.pi * x) + 2 * x\n",
    "\n",
    "x_range = jnp.array([jnp.arange(0,4,.01)]).T\n",
    "key, subkey = jax.random.split(key)\n",
    "x = 2 * jax.random.uniform(subkey, (N, D))\n",
    "w = 2 * jnp.ones((D,))\n",
    "key, subkey = jax.random.split(key)\n",
    "y = f(x[:,0]) + sigma_y * jax.random.normal(subkey, (N,))\n",
    "y = jnp.array([y]).T\n",
    "y_range = f(x_range)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(x, y, 'ks')\n",
    "plt.plot(x_range, y_range, 'k')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "For such a case, the `GT` provides a new conditional object `NNControlGaussianConditional`\n",
    "\n",
    "$$\n",
    "    p_\\phi(Y\\vert X,u) = {\\cal N}(\\mu_\\phi(X,u), \\Sigma),\n",
    "$$\n",
    "\n",
    "where $\\mu_\\phi(X,u)=M(u)X + b(u)$ is a neural network with parameters $\\phi$. Let's construct such an object for the data we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "sigma_y = 0.2\n",
    "mu_prior = 0\n",
    "sigma_prior = 1.0\n",
    "\n",
    "# Construct prior\n",
    "Sigma = jnp.array([sigma_prior ** 2.0 * jnp.eye(D)])\n",
    "mu = mu_prior * jnp.ones((1, D))\n",
    "prior = pdf.GaussianDiagPDF(Sigma=Sigma, mu=mu) # p(W)\n",
    "\n",
    "# Construct likelihood p_u(Y_i| X=x_i, W)\n",
    "Sigma_lk = jnp.ones((1, 1, 1)) * sigma_y ** 2\n",
    "Dy, Dx = 1, 1\n",
    " \n",
    "def get_neg_log_marginal_llk(X,y):\n",
    "    mlp = hk.nets.MLP([10, Dy * (Dx + 1)], activation=jax.nn.tanh)\n",
    "    likelihood = conditional.NNControlGaussianConditional(Sigma=Sigma_lk, num_cond_dim=Dx, num_control_dim=Dx, control_func=mlp)\n",
    "    # p(Y|W,U=x)\n",
    "    likelihood_control = likelihood.set_control_variable(X)\n",
    "    # p(Y=y|W,U=x)\n",
    "    likelihood_product = likelihood_control.set_y(y).product()\n",
    "    # p(Y=y,W|U=x) = p(Y=y|W,U=x)p(W)\n",
    "    posterior_unnormalized = prior * likelihood_product\n",
    "    # - log p(Y=y|U=x) = - log \\int p(Y=y,W|U=x) dW\n",
    "    log_marg_likelihood = posterior_unnormalized.log_integral()\n",
    "    return -log_marg_likelihood.squeeze()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The only difference to the previous linear example is, that before using the `NNControlGaussianConditional` we need to set the control variables, which then returns a `ConditionalGaussianPDF` again.\n",
    "\n",
    "However, we also need to learn the hyperparameters $\\phi$. This can be done by minimizing the negative log marginal likelihood defined as\n",
    "\n",
    "$$\n",
    "    -\\ell(\\phi) = -\\log \\int p_\\phi(Y=y\\vert W, u=x)p(W){\\rm d}W.\n",
    "$$\n",
    "\n",
    "As we see below, this can be computed in just a few lines in `GT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian_toolbox.utils.jax_minimize_wrapper import minimize\n",
    "key = jax.random.PRNGKey(42)\n",
    "neg_llk_t = hk.transform(get_neg_log_marginal_llk)\n",
    "neg_llk_t = hk.without_apply_rng(neg_llk_t)\n",
    "params = neg_llk_t.init(key, x, y)\n",
    "result  = minimize(neg_llk_t.apply, params, 'L-BFGS-B', args=(x,y))\n",
    "params_opt = result.x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Since `GT` is based upon `jax` and `objax` optimization of $\\phi$ is quite straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now with the optimal $\\phi$ we just do inference as before, with the only difference, that we need to set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET POSTERIOR\n",
    "\n",
    "mlp_t = hk.without_apply_rng(hk.transform(lambda x: hk.nets.MLP([10, Dy * (Dx + 1)], activation=jax.nn.tanh)(x)))\n",
    "mlp_opt = lambda x: mlp_t.apply(params_opt, x)\n",
    "likelihood = conditional.NNControlGaussianConditional(Sigma=Sigma_lk, num_cond_dim=Dx, num_control_dim=Dx, control_func=mlp_opt)\n",
    "\n",
    "# p(Y|W,U=x)\n",
    "likelihood_control = likelihood.set_control_variable(x)\n",
    "# p(Y=y|W,U=x)\n",
    "likelihood_product = likelihood_control.set_y(y).product()\n",
    "# p(Y=y,W|U=x) = p(Y=y|W,U=x)p(W)\n",
    "posterior_unnormalized = prior * likelihood_product\n",
    "# p(W|Y=y, U=x) = p(Y=y,W|U=x)/int p(Y=y,W|U=x) dW\n",
    "posterior = posterior_unnormalized.get_density()\n",
    "\n",
    "# MAKE PREDICTIONS\n",
    "# p(Y*|W,U*=x*)\n",
    "predictive_conditional = likelihood.set_control_variable(x_range)\n",
    "# p(Y*|U*=x*, Y=y, U=x) = \\int p(Y*|W,U=x*)p(W|Y=y, U=x) dW\n",
    "predictive_posterior = predictive_conditional.affine_marginal_transformation(posterior)\n",
    "\n",
    "# PLOTTING\n",
    "plt.plot(x, y, 'ks')\n",
    "plt.plot(x_range, y_range, 'k')\n",
    "plt.plot(x_range, predictive_posterior.mu, 'C3--')\n",
    "plt.fill_between(x_range[:,0], predictive_posterior.mu[:,0] - 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                              predictive_posterior.mu[:,0] + 2 * jnp.sqrt(predictive_posterior.Sigma[:,0,0]),\n",
    "                alpha=.3, color='C3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Of course, this is just a simple example. One more complex example is to paramterize also the covariance $\\Sigma$ via control variables. You want to try it yourself? ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gaussian_toolbox')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "782cd5dadb26a54d425c88a99e91aaf22c15a8b7b364f1b6867d338206b7ed04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
